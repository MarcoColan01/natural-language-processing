{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9e83a9",
   "metadata": {},
   "source": [
    "##### Master Degree in Computer Science and Data Science for Economics\n",
    "\n",
    "# Biases and stereotypes\n",
    "\n",
    "### Alfio Ferrara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cbd92",
   "metadata": {},
   "source": [
    "## Working definitions\n",
    "\n",
    "**Bias**: Bias is a tendency or preference that causes unfair or unbalanced outcomes.\n",
    "In AI and machine learning, bias happens when a model makes decisions that systematically favor or disadvantage certain groups, ideas, or outcomes due to the data it was trained on or how it was designed.\n",
    "\n",
    "**Stereotype**: A stereotype is a fixed and oversimplified idea about a group of people.\n",
    "It assumes that everyone in that group is the same, based on things like race, gender, age, or job, without considering individual differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f798c",
   "metadata": {},
   "source": [
    "Let's start exploring a simple example of how biases can enforce stereotypes in text classification.\n",
    "\n",
    "The example is available in [L8.2-bias-bert-classifier](./L8.2-bias-bert-classifier.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec140b4",
   "metadata": {},
   "source": [
    "## Checking for biases in LLM (just some simple examples)\n",
    "\n",
    "- [BERT masking language](./L8.6-bias-masking-task.ipynb)\n",
    "- [ChatGPT 3.5](./L8.7-bias-completion-task.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24abf0b2",
   "metadata": {},
   "source": [
    "These are just simplistic examples. Bias detection is a more complex tasks that combines also explainability methods to investigate the \"inside\" of the models. For some references check\n",
    "\n",
    "> Naous, T., Ryan, M. J., Ritter, A., & Xu, W. (2023). Having beer after prayer? measuring cultural bias in large language models. arXiv preprint arXiv:2305.14456.\n",
    "\n",
    "> Li, C., Chen, M., Wang, J., Sitaram, S., & Xie, X. (2024). Culturellm: Incorporating cultural differences into large language models. Advances in Neural Information Processing Systems, 37, 84799-84838."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05907ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
