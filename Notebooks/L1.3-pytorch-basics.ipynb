{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64418514",
   "metadata": {},
   "source": [
    "##### Master Degree in Computer Science and Data Science for Economics\n",
    "\n",
    "# Primer about PyTorch\n",
    "\n",
    "### Davide Riva (materials from Darya Shlyk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"align: center;\"><img src=\"http://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png\" width=400 height=100></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 style=\"text-align: center;\"><b>Installing PyTorch</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to [pytorch.org](https://pytorch.org/) and install PyTorch version choosing your system preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu129\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a deep learning framework, build on 3 main components.\n",
    "\n",
    "$$ PyTorch = NumPy + CUDA +Autograd$$\n",
    "\n",
    "PyTorch works with ``tensor`` data structure, which is similar to NumPy's ``ndarray``, except that:\n",
    " 1. tensors can run on GPUs or other hardware accelerators via [CUDA](http://en.wikipedia.org/wiki/CUDA)\n",
    " 2. tensors are also optimized for automatic differentiation (Autograd)\n",
    "\n",
    "Tensor is a multi-dimensional array:\n",
    "- a scalar is a zero dimensional tensor : `` x = 9 # shape ([])``\n",
    "- a vector, is a 1d tensor : `` y = [9, 5, 10] # shape ([3])``\n",
    "- a matrix is a 2d tensor : `` z = [[9, 5, 10], [12, 6, 3]] # shape ([2, 3])``\n",
    "- a 3d array is a 3d tensor: `` t = [[[9, 5, 10], [12, 6, 3]], [[7, 4, 11], [2, 13, 8]]] # shape ([2, 2, 3])``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 style=\"text-align: center;\"><b> Creating a tensor</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to create a tensor, depending on your use case:\n",
    "1. directly from pre-existing data, like Python list or any sequence, NumPy array, etc.\n",
    "2. creating a tensor with a specific shape, filled with random or constant values\n",
    "3. from another tensor, keeping its shape and type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To create a tensor with pre-existing data, use ``torch.tensor()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "data =[1, 2, 3]                         # A vector\n",
    "#matrix =[[1, 2, 3], [4,5,6]]           # A matrix\n",
    "#cube =[[[1, 2, 3], [4,5,6], [7,8,9]]]  # A 3D tensor\n",
    "x = torch.tensor(data, dtype=torch.float32, requires_grad=True) # The data type is automatically inferred.\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From NumPy to PyTorch and viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_array = np.array(data)\n",
    "\n",
    "t = torch.from_numpy(np_array)    # creates a tensor that shares storage with a NumPy array\n",
    "t.numpy()                         # back to numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a tensor with specific size, filled with random or constant values, use ``torch.* ``,\\\n",
    "passing a sequence of integers defining the desired ``shape`` of a tensor.\\\n",
    "``shape`` can be a variable number of integers OR a collection like a list or tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3, 2, 4) #I want a 3D tensor with 3 matrices, each with 2 rows and 4 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling values uniformly from [0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0012, 0.8099, 0.9770, 0.4028],\n",
      "         [0.2935, 0.4790, 0.0060, 0.3887]],\n",
      "\n",
      "        [[0.0328, 0.6195, 0.4360, 0.4125],\n",
      "         [0.6314, 0.8177, 0.7051, 0.7215]],\n",
      "\n",
      "        [[0.6758, 0.4331, 0.0682, 0.8136],\n",
      "         [0.3367, 0.5916, 0.6288, 0.6613]]])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "random_uniform = torch.rand(shape)                          # FloatTensor from Uniform[0, 1)\n",
    "print(random_uniform) \n",
    "print(random_uniform.size())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling integers uniformly from [low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6, 5, 6, 6],\n",
      "         [3, 4, 4, 6]],\n",
      "\n",
      "        [[5, 2, 5, 6],\n",
      "         [6, 5, 2, 3]],\n",
      "\n",
      "        [[6, 3, 6, 4],\n",
      "         [2, 3, 4, 2]]])\n"
     ]
    }
   ],
   "source": [
    "random_uniform = torch.randint(low=2, high=7, size=shape, device=\"cpu\")   # IntTensor from Uniform[2, 7)\n",
    "print(random_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.1127, 4.7473],\n",
      "        [5.4954, 5.1634],\n",
      "        [6.8389, 2.4585]])\n"
     ]
    }
   ],
   "source": [
    "shape = (3, 2)\n",
    "random_normal = torch.randn(shape)                          # FloatTensor from Normal(0, 1) \n",
    "random_normal = torch.normal(mean=5, std=2, size=shape)     # FloatTensor from Normal(5, 2)\n",
    "print(random_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy tensors of zeros and ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "zero_tensor = torch.zeros(shape)                             # initialize with zeros\n",
    "ones_tensor = torch.ones(shape)                              # initialize with ones\n",
    "print(ones_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors of a specific data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False],\n",
      "        [False, False],\n",
      "        [False, False]])\n"
     ]
    }
   ],
   "source": [
    "# shape MUST be a variable number of integers\n",
    "float_tensor = torch.FloatTensor(3, 2)                    # dtype = float32 == torch.Tensor(shape)\n",
    "int_tensor = torch.IntTensor(3, 2)                        # dtype = int32\n",
    "long_tensor = torch.LongTensor(3, 2)                      # dtype = int64\n",
    "bool_tensor = torch.BoolTensor(3, 2)                      # dtype = bool\n",
    "print(bool_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a tensor with the same size and type as another tensor, use ``torch.*_like`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(data, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "new_tensor = torch.randn_like(t)                          # retains the shape and dtype of t\n",
    "new_tensor = torch.rand_like(t)                           # retains the shape and dtype of t\n",
    "new_tensor = torch.ones_like(t)                           # retains the shape and dtype of t\n",
    "new_tensor = torch.zeros_like(t, dtype=torch.int)         # can override the datatype \n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\"><b> Changing the content of existing tensor </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any PyTorch method with an underscore ``_`` modifies the object in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zero_()                   # zero out tensor's content\n",
    "t.uniform_()                # replace with values from a continuous random Uniform[from, to)\n",
    "t.normal_(mean=3)           # replace with values from a Normal(mean, std) \n",
    "t.fill_(3)                  # fill-in tensor with a constant value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 style=\"text-align: center;\"><b> Attributes of a Tensor </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n",
      "Shape: torch.Size([3])\n",
      "Type: torch.FloatTensor\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(t)\n",
    "print(\"Shape:\",   t.size()) # same as t.shape\n",
    "print(\"Type:\",    t.type())\n",
    "print(\"Device:\",  t.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\"><b> Changing tensor's type </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n",
      "Type: torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "t = t.to(torch.float)\n",
    "print(t)\n",
    "print(\"Type:\", t.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing tensor's device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default all tensors are allocated on the CPU memory, where all the computation is performed.\\\n",
    "If you have a GPU, you can access its memory via a specialized API called CUDA.\\\n",
    "Move tensors from cpu to cuda and viceversa by using the ``.to(device)`` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "MPS: False\n",
      "MTIA: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA: {torch.cuda.is_available()}\") # Nvidia CUDA\n",
    "print(f\"MPS: {torch.mps.is_available()}\")   # Apple MPS\n",
    "print(f\"MTIA: {torch.mtia.is_available()}\") # Meta MTIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move tensor to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "t = t.to(device) # t.cpu() or t.cuda()\n",
    "print(\"Device:\",  t.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform computations on GPU and move the results to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor([[-4.8314, -3.4880,  0.9040,  ..., -0.1431, -5.5325, -1.0214],\n",
      "        [ 1.3142, -0.2971, -1.3260,  ..., -2.0403,  2.1070,  1.2908],\n",
      "        [ 0.6699, -0.3230, -1.3532,  ...,  0.2824, -1.3489, -0.3285],\n",
      "        ...,\n",
      "        [-2.8871,  0.6629, -1.5071,  ...,  0.9398, -0.4522, -3.5498],\n",
      "        [-0.7695,  3.4103,  1.6930,  ..., -0.4491,  0.0402, -0.9081],\n",
      "        [-3.5212, -0.1739,  1.2079,  ...,  2.3413,  0.0781,  3.6768]])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(100, 100).to(device)\n",
    "b = torch.clone(a).to(device)\n",
    "c = (a + b)\n",
    "print(c.device)\n",
    "c = c.cpu()\n",
    "n = c.numpy()\n",
    "print(c)\n",
    "print(c.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To perform computations all tensors need to be on the same device !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 style=\"text-align: center;\"><b> Reshaping a tensor </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(0, 10, step=1) # [start=0, end=10, step=1)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape from 1d to 2d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``[10] -> [2, 5]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = t.view(2, 5) # y shares the storage with the original tensor t, cannot operate on tensors that are not contiguous in memory\n",
    "y = t.reshape(2, -1) # y may share the storage with t (y is a view of t) or not ( y is a copy of t), thus reshape can operate on tensors that are either contiguous or not contiguous in memory\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34f55634",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m      3\u001b[0m z \u001b[38;5;241m=\u001b[39m x[:, ::\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# Example of an error from view\n",
    "x = torch.randn(2, 4, 8)\n",
    "z = x[:, ::2]\n",
    "z.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new dimension of size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``[2, 5] -> [2, 5, 1]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([2, 5, 1])\n",
      "tensor([[[0],\n",
      "         [1],\n",
      "         [2],\n",
      "         [3],\n",
      "         [4]],\n",
      "\n",
      "        [[5],\n",
      "         [6],\n",
      "         [7],\n",
      "         [8],\n",
      "         [9]]])\n"
     ]
    }
   ],
   "source": [
    "b = y.unsqueeze(2) # same as y.unsqueeze(-1) where -1 means the last dimension \n",
    "print(\"Shape:\", b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand dimension(s) of size 1 to a larger size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``[2, 5, 1] -> [2, 5, 10]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "         [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]],\n",
       "\n",
       "        [[5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "         [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "         [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "         [8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       "         [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.expand(-1, -1, 10) # -1 means not changing the size of that dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exchange the order of dimensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``[2, 5, 1] -> [1, 5, 2]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 5, 2])\n",
      "tensor([[[0, 5],\n",
      "         [1, 6],\n",
      "         [2, 7],\n",
      "         [3, 8],\n",
      "         [4, 9]]])\n"
     ]
    }
   ],
   "source": [
    "x = b.transpose(0, 2)    # == y.transpose(2, 0) specify 2 dimensions to swap\n",
    "x = b.permute(2, 1, 0)   # specify the desired ordering for all dimensions\n",
    "print(\"Shape:\", x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove dimension(s) of size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``[1, 5, 2] -> [2, 5]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([5, 2])\n",
      "tensor([[0, 5],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [3, 8],\n",
      "        [4, 9]])\n"
     ]
    }
   ],
   "source": [
    "y = x.squeeze(0)\n",
    "print(\"Shape:\", y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``[2, 5] -> [10]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 5, 1, 6, 2, 7, 3, 8, 4, 9])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten\n",
    "t = y.reshape(-1) # or t.flatten()\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 style=\"text-align: center;\"><b> Arithmetic operations on tensors </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Python operator | PyTorch method |\n",
    "|:-:|:-:|\n",
    "|+| torch.add() |\n",
    "|-| torch.sub() |\n",
    "|*| torch.mul() |\n",
    "|/| torch.div() |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise operations on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + t\n",
    "t - t\n",
    "t * t\n",
    "t / t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  -inf, 1.6094, 0.0000, 1.7918, 0.6931, 1.9459, 1.0986, 2.0794, 1.3863,\n",
       "        2.1972])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.4841e+02, 2.7183e+00, 4.0343e+02, 7.3891e+00, 1.0966e+03,\n",
       "        2.0086e+01, 2.9810e+03, 5.4598e+01, 8.1031e+03])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 2.2361, 1.0000, 2.4495, 1.4142, 2.6458, 1.7321, 2.8284, 2.0000,\n",
       "        3.0000])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 25,  1, 36,  4, 49,  9, 64, 16, 81])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.pow(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 style=\"text-align: center;\"><b>[Dot product vs Matrix Multiplication](https://mkang32.github.io/python/2020/08/30/numpy-matmul.html) </b>\n",
    "\n",
    "### Dot product\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_1 & a_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\ b_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "a_1 b_1 + a_2 b_2\n",
    "$$\n",
    "\n",
    "### Matrix multiplication\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n",
    "a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Computes the inner product of 1D tensors.\\\n",
    " ``(10) @ (10) -> (1) ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor(56.) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor([2, 4, 6])\n",
    "\n",
    "print(t.size())\n",
    "print(t.dot(t), t.dot(t).size())                                           # same as (t * t).sum()                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Performs a matrix multiplication of 2D tensors.\\\n",
    " **Number of columns in the first tensor should match up with the number of rows in the second tensor!** \\\n",
    " ``(2, 5) @ (5, 2) -> (2, 2) ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 30,  80],\n",
       "        [ 80, 255]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(y.transpose(0, 1), y)                      # same as y.T @ y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a83a3e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 30,  80],\n",
       "        [ 80, 255]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.T @ y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs a batch matrix-matrix product of 3D tensors. \\\n",
    "``(2, 5, 1) @ (2, 1, 5) -> (2, 5, 5)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0,  0,  0,  0],\n",
       "         [ 0,  1,  2,  3,  4],\n",
       "         [ 0,  2,  4,  6,  8],\n",
       "         [ 0,  3,  6,  9, 12],\n",
       "         [ 0,  4,  8, 12, 16]],\n",
       "\n",
       "        [[25, 30, 35, 40, 45],\n",
       "         [30, 36, 42, 48, 54],\n",
       "         [35, 42, 49, 56, 63],\n",
       "         [40, 48, 56, 64, 72],\n",
       "         [45, 54, 63, 72, 81]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_product = torch.bmm(b , b.transpose(1, 2))    # same as b @ b.transpose(1, 2)\n",
    "batch_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universal solution for matrix multiplication<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``torch.matmul(tensor1, tensor2)`` a PyTorch equivalent of Python operator ``@``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0,  0,  0,  0],\n",
       "         [ 0,  1,  2,  3,  4],\n",
       "         [ 0,  2,  4,  6,  8],\n",
       "         [ 0,  3,  6,  9, 12],\n",
       "         [ 0,  4,  8, 12, 16]],\n",
       "\n",
       "        [[25, 30, 35, 40, 45],\n",
       "         [30, 36, 42, 48, 54],\n",
       "         [35, 42, 49, 56, 63],\n",
       "         [40, 48, 56, 64, 72],\n",
       "         [45, 54, 63, 72, 81]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(t, t)                                  # same as torch.dot(t, t)\n",
    "torch.matmul(y.T, y)                                # same as torch.mm(y.T, y)\n",
    "torch.matmul(b, b.transpose(1, 2))                  # same as torch.bmm(b, b.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 style=\"text-align: center;\"><b> Aggregating operations on tensors </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\"><b> sum (dim) / mean (dim) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  10,  20,  30,  40],\n",
       "        [175, 210, 245, 280, 315]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_product.sum(dim=-1)            # collapsing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  10,  20,  30,  40],\n",
       "        [175, 210, 245, 280, 315]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_product.sum(dim=-2)            # collapsing rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aggregating, you may end up with a one-element tensor,\\\n",
    "you can convert it to a Python numerical value using ``item()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_product.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\"><b> max (dim) / min (dim)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a **namedtuple (values, indices)** \\\n",
    "where ``values`` is the maximum/minimum value of each row of the tensor in a given dimension.\\\n",
    "And ``indices`` is the index of each maximum/minimum value found (``argmin``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  4,  8, 12, 16],\n",
      "        [45, 54, 63, 72, 81]])\n",
      "tensor([[0, 4, 4, 4, 4],\n",
      "        [4, 4, 4, 4, 4]])\n"
     ]
    }
   ],
   "source": [
    "values, indices = batch_product.max(dim=1)\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(81)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_product.max() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\"><b> argmax (dim) / argmin (dim)</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the **indices** of the maximum/minimum values of a tensor across a dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 4, 4, 4, 4],\n",
       "        [4, 4, 4, 4, 4]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_product.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d54db35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0,  0,  0,  0],\n",
       "         [ 0,  1,  2,  3,  4],\n",
       "         [ 0,  2,  4,  6,  8],\n",
       "         [ 0,  3,  6,  9, 12],\n",
       "         [ 0,  4,  8, 12, 16]],\n",
       "\n",
       "        [[25, 30, 35, 40, 45],\n",
       "         [30, 36, 42, 48, 54],\n",
       "         [35, 42, 49, 56, 63],\n",
       "         [40, 48, 56, 64, 72],\n",
       "         [45, 54, 63, 72, 81]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\"><b>topk ( k, dim, largest )</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a **namedtuple of (values, indices)** \\\n",
    "with values and indices of ``k`` largest/smallest elements of a tensor along ``dim``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: tensor([[[ 0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[25, 30, 35, 40, 45]]])\n",
      "Indices: tensor([[[0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "values, indices = batch_product.topk(k=1, dim=1, largest=False)\n",
    "print(\"Values:\", values)\n",
    "print(\"Indices:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><b> Standard NumPy indexing and slicing </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([0, 1, 2])\n",
      "First column: tensor([ 0,  3,  6,  9, 12])\n",
      "Last column: tensor([ 2,  5,  8, 11, 14])\n",
      "tensor([[ 0,  0,  2],\n",
      "        [ 3,  0,  5],\n",
      "        [ 6,  0,  8],\n",
      "        [ 9,  0, 11],\n",
      "        [12,  0, 14]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(15).view(5, 3)\n",
    "\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "\n",
    "tensor[:,1] = 0 # zero-out second column\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><b> Masked indexing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[ 0, -1,  2],\n",
      "        [-3,  4, -5],\n",
      "        [ 6,  7,  8]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(9)\n",
    "b = torch.clone(a)      # creates a copy of tensor a\n",
    "\n",
    "# generating random indices and performing assignment \n",
    "negative_indices = torch.randint(low=0, high=9, size=(4,))\n",
    "b[negative_indices] = b[negative_indices] * (-1) \n",
    "\n",
    "# reshaping from 1d to 2d\n",
    "a = a.view(3, 3)\n",
    "b = b.view(3, 3)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computes element-wise equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [False,  True, False],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.eq(b)                   # same as a == b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes element-wise disequality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False],\n",
       "        [ True, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.ne(b)                    # same as a != b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting elements via boolean mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -3, -5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[a > b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 style=\"text-align: center;\"><b> Joining multiple tensors </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create 2 tensors \n",
    "t1 = torch.arange(6).view(2, 3)\n",
    "t2 = torch.arange(6).view(2, 3)\n",
    "t1, t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\"><b>torch.cat ( (tensors), dim=0 )</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat tensors one on top of the other (default):\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Concat tensors one on top of the other (default):\")\n",
    "print(torch.cat((t1, t2), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat tensors side by side:\n",
      "tensor([[0, 1, 2, 0, 1, 2],\n",
      "        [3, 4, 5, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Concat tensors side by side:\")\n",
    "print(torch.cat((t1, t2), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\"><b>torch.stack ( (tensors), dim=0 )</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``dim`` is a new dimension to insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [3, 4, 5]],\n",
       "\n",
       "        [[0, 1, 2],\n",
       "         [3, 4, 5]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((t1, t2), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference between ``torch.stack`` and ``torch.cat``** :\\\n",
    "``torch.stack`` creates a new dimension to stack tensors, while ``torch.cat`` not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 style=\"text-align: center;\"><b> Practice </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a 2D tensor and add a batch dimension of size 1\n",
    "2. Create a random tensor of shape 5x3 in the interval [3, 7)\n",
    "3. Create a tensor with values from a normal with mean=0, std=3\n",
    "4. Perform a batch product between 3D tensors\n",
    "5. Return a batch matrix product between a 3D tensor and a 2D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15c26e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
