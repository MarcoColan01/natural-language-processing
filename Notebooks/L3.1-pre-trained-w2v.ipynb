{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Master Degree in Computer Science and Data Science for Economics\n",
    "\n",
    "# Word2Vec resources\n",
    "## Example of using W2V to check for semantic shifts\n",
    "\n",
    "### Alfio Ferrara\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functionalities of `gensim` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()['cousine']\n",
    "recipes = db['foodcom']\n",
    "\n",
    "q = {}\n",
    "recipe_corpus = []\n",
    "size = recipes.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for recipe in recipes.find(q).limit(limit):\n",
    "    try:\n",
    "        recipe_corpus.append(word_tokenize(recipe['description'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'love', 'grits', ',', 'this', 'is', 'another', 'good', 'way', 'to', 'serve', 'them', '.', 'a', 'great', 'alternative', 'to', 'a', 'baked', 'potato', 'when', 'served', 'with', 'grilled', 'steak', 'or', 'chicken', '.', 'i', 'belive', 'this', 'recipe', 'could', 'be', 'made', 'with', 'instant', 'grits.the', '2', '1/2', 'hours', 'for', 'refrigeration', 'is', 'not', 'include', 'in', 'time', '.', 'the', 'recipe', 'comes', 'from', 'tast', 'of', 'home', \"'s\", 'light', 'and', 'tasty', '.']\n"
     ]
    }
   ],
   "source": [
    "print(recipe_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_model = Word2Vec(sentences=recipe_corpus, vector_size=300, window=5, \n",
    "                        min_count=1, workers=8, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supper', 0.6802205443382263),\n",
       " ('meal', 0.5461012125015259),\n",
       " ('brunch', 0.47505271434783936),\n",
       " ('appetizer', 0.4526340365409851),\n",
       " ('entree', 0.45230329036712646),\n",
       " ('entertaining', 0.4501704275608063),\n",
       " ('dinners', 0.429585725069046),\n",
       " ('gathering', 0.4263454079627991),\n",
       " ('lunch', 0.42482757568359375),\n",
       " ('meals', 0.41012176871299744)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compositionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = recipe_model.wv.doesnt_match(['pasta', 'spaghetti', 'noodles', 'apple'])\n",
    "common = recipe_model.wv.get_mean_vector(['pasta', 'spaghetti', 'noodles', 'risotto'])\n",
    "common_word = recipe_model.wv.similar_by_vector(common)\n",
    "analogy = recipe_model.wv.most_similar(positive=['pizza', 'steak'], negative=['tomato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doesn't match: apple\n",
      "Common terms: [('pasta', 0.7843869924545288), ('noodles', 0.783257782459259), ('spaghetti', 0.7344835996627808), ('risotto', 0.601172924041748), ('lasagna', 0.5741502642631531), ('polenta', 0.5542970299720764), ('linguine', 0.5229288935661316), ('fettuccine', 0.5191859602928162), ('penne', 0.518997848033905), ('couscous', 0.508514404296875)]\n",
      "Analogy: [('grill', 0.38019317388534546), ('bbq', 0.35111916065216064), ('burger', 0.34664222598075867), ('steaks', 0.340492844581604), ('lasagna', 0.3275046646595001), ('reuben', 0.32319068908691406), ('ribs', 0.3157573342323303), ('broiling', 0.3092014491558075), ('fajitas', 0.30499404668807983), ('indoor', 0.30170127749443054)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Doesn't match: {dm}\")\n",
    "print(f\"Common terms: {common_word}\")\n",
    "print(f\"Analogy: {analogy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models vectors to measure a shift in meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tv = pymongo.MongoClient()['tmdb']\n",
    "tvseries = db_tv['tvseries']\n",
    "\n",
    "q = {}\n",
    "tv_corpus = []\n",
    "size = tvseries.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for tvs in tvseries.find(q).limit(limit):\n",
    "    try:\n",
    "        tv_corpus.append(word_tokenize(tvs['overview'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walter', 'white', ',', 'a', 'new', 'mexico']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_corpus[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_model = Word2Vec(sentences=tv_corpus, vector_size=100, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ultimate', 0.9939123392105103),\n",
       " ('phone', 0.9924060702323914),\n",
       " ('scoops', 0.9913731217384338),\n",
       " ('maria', 0.9903990030288696),\n",
       " ('clarity', 0.9899325966835022),\n",
       " ('miami', 0.9899277091026306),\n",
       " ('skywalker', 0.9898713827133179),\n",
       " ('mace', 0.9897506237030029),\n",
       " ('mei', 0.9896987676620483),\n",
       " ('fan', 0.9895042777061462)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_model.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sister', 0.7777917981147766),\n",
       " ('daughter', 0.7555021643638611),\n",
       " ('boyfriend', 0.7444507479667664),\n",
       " ('niece', 0.7425244450569153),\n",
       " ('wife', 0.7346354722976685),\n",
       " ('dd', 0.7327642440795898),\n",
       " ('dad', 0.7315940856933594),\n",
       " ('fiance', 0.728138267993927),\n",
       " ('son', 0.7229806780815125),\n",
       " ('brother-in-law', 0.7174952626228333)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('brother')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring semantic shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian corpus: 4920, Chinese corpus: 4871\n"
     ]
    }
   ],
   "source": [
    "italian_q = {'search_terms': 'italian'}\n",
    "chinese_q = {'search_terms': 'chinese'}\n",
    "limit = 5_000\n",
    "italian_corpus = []\n",
    "chinese_corpus = []\n",
    "\n",
    "for q, c in [(italian_q, italian_corpus), (chinese_q, chinese_corpus)]:\n",
    "    for doc in recipes.find(q).limit(limit):\n",
    "        try:\n",
    "            tokens = word_tokenize(doc['description'].lower())\n",
    "            c.append(tokens)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "print(f\"Italian corpus: {len(italian_corpus)}, Chinese corpus: {len(chinese_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus = italian_corpus + chinese_corpus\n",
    "m0 = Word2Vec(sentences=main_corpus, vector_size=100, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.658794105052948),\n",
       " ('supper', 0.6461277008056641),\n",
       " ('lunch', 0.5710224509239197),\n",
       " ('night', 0.5304909944534302),\n",
       " ('multitasking', 0.5162426829338074),\n",
       " ('snack', 0.5153259038925171),\n",
       " ('brunch', 0.5014334321022034),\n",
       " ('dinners', 0.49514809250831604),\n",
       " ('cocktail', 0.4790607988834381),\n",
       " ('take-alongs', 0.47777262330055237)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m0.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the global model to specific sub-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_it = copy.deepcopy(m0)\n",
    "m_ch = copy.deepcopy(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7022862, 9992950)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.train(italian_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)\n",
    "m_ch.train(chinese_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.5930757522583008),\n",
       " ('supper', 0.546654224395752),\n",
       " ('snack', 0.5166825652122498),\n",
       " ('cocktail', 0.4984075427055359),\n",
       " ('multitasking', 0.4908176362514496),\n",
       " ('hostess', 0.46310943365097046),\n",
       " ('take-alongs', 0.45842233300209045),\n",
       " ('brunch', 0.45402300357818604),\n",
       " ('lunch', 0.45054692029953003),\n",
       " ('picnics', 0.4429490268230438)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.6648055911064148),\n",
       " ('supper', 0.5536285638809204),\n",
       " ('multitasking', 0.49264785647392273),\n",
       " ('night', 0.48381927609443665),\n",
       " ('lunch', 0.48119181394577026),\n",
       " ('snack', 0.46922823786735535),\n",
       " ('crowd', 0.43798351287841797),\n",
       " ('entree', 0.41686946153640747),\n",
       " ('entertaining', 0.4125153720378876),\n",
       " ('unexpected', 0.40737923979759216)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_ch.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'spaghetti'\n",
    "v0, vit, vch = m0.wv.get_vector(word), m_it.wv.get_vector(word), m_ch.wv.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to IT: 0.05137380935261615\n",
      "Moving to CH: 0.05520601573640993\n",
      "Moving from IT to CH: 0.09825096112418708\n"
     ]
    }
   ],
   "source": [
    "print(f\"Moving to IT: {distance.cosine(vit, v0)}\")\n",
    "print(f\"Moving to CH: {distance.cosine(vch, v0)}\")\n",
    "print(f\"Moving from IT to CH: {distance.cosine(vch, vit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
