{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Master Degree in Computer Science and Data Science for Economics\n",
    "\n",
    "# Word2Vec resources\n",
    "## Example of using W2V to check for semantic shifts\n",
    "\n",
    "### Alfio Ferrara\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.1/24.0 MB 10.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.2/24.0 MB 10.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.8/24.0 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.1/24.0 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.7/24.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 13.9/24.0 MB 11.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.8/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 18.9/24.0 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.7/24.0 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.0 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.6/15.5 MB 13.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 4.5/15.5 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.9/15.5 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.5 MB 15.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.4/15.5 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 13.0 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)\n",
      "   ---------------------------------------- 0.0/45.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.1/45.9 MB 10.7 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 4.2/45.9 MB 10.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 6.8/45.9 MB 10.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 8.4/45.9 MB 9.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 10.7/45.9 MB 10.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 13.9/45.9 MB 10.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 16.0/45.9 MB 10.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 17.8/45.9 MB 10.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 18.9/45.9 MB 10.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 19.9/45.9 MB 9.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 19.9/45.9 MB 9.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 23.1/45.9 MB 9.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 24.6/45.9 MB 9.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 28.8/45.9 MB 9.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 31.7/45.9 MB 10.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 33.3/45.9 MB 9.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 36.2/45.9 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.7/45.9 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.7/45.9 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.8/45.9 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.9/45.9 MB 9.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.6/45.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.9/45.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.9/45.9 MB 9.2 MB/s eta 0:00:00\n",
      "Using cached smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.3.1 wrapt-1.17.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functionalities of `gensim` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()['cousine']\n",
    "recipes = db['foodcom']\n",
    "\n",
    "q = {}\n",
    "recipe_corpus = []\n",
    "size = recipes.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for recipe in recipes.find(q).limit(limit):\n",
    "    try:\n",
    "        recipe_corpus.append(word_tokenize(recipe['description'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'love', 'grits', ',', 'this', 'is', 'another', 'good', 'way', 'to', 'serve', 'them', '.', 'a', 'great', 'alternative', 'to', 'a', 'baked', 'potato', 'when', 'served', 'with', 'grilled', 'steak', 'or', 'chicken', '.', 'i', 'belive', 'this', 'recipe', 'could', 'be', 'made', 'with', 'instant', 'grits.the', '2', '1/2', 'hours', 'for', 'refrigeration', 'is', 'not', 'include', 'in', 'time', '.', 'the', 'recipe', 'comes', 'from', 'tast', 'of', 'home', \"'s\", 'light', 'and', 'tasty', '.']\n"
     ]
    }
   ],
   "source": [
    "print(recipe_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m recipe_model \u001b[38;5;241m=\u001b[39m \u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecipe_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cola0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_total_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cola0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[0;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[0;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cola0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_epoch_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_corpus_file_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[1;32mc:\\Users\\cola0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cola0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cola0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recipe_model = Word2Vec(sentences=recipe_corpus, vector_size=300, window=5, \n",
    "                        min_count=1, workers=8, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supper', 0.6802205443382263),\n",
       " ('meal', 0.5461012125015259),\n",
       " ('brunch', 0.47505271434783936),\n",
       " ('appetizer', 0.4526340365409851),\n",
       " ('entree', 0.45230329036712646),\n",
       " ('entertaining', 0.4501704275608063),\n",
       " ('dinners', 0.429585725069046),\n",
       " ('gathering', 0.4263454079627991),\n",
       " ('lunch', 0.42482757568359375),\n",
       " ('meals', 0.41012176871299744)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compositionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = recipe_model.wv.doesnt_match(['pasta', 'spaghetti', 'noodles', 'apple'])\n",
    "common = recipe_model.wv.get_mean_vector(['pasta', 'spaghetti', 'noodles', 'risotto'])\n",
    "common_word = recipe_model.wv.similar_by_vector(common)\n",
    "analogy = recipe_model.wv.most_similar(positive=['pizza', 'steak'], negative=['tomato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doesn't match: apple\n",
      "Common terms: [('pasta', 0.7843869924545288), ('noodles', 0.783257782459259), ('spaghetti', 0.7344835996627808), ('risotto', 0.601172924041748), ('lasagna', 0.5741502642631531), ('polenta', 0.5542970299720764), ('linguine', 0.5229288935661316), ('fettuccine', 0.5191859602928162), ('penne', 0.518997848033905), ('couscous', 0.508514404296875)]\n",
      "Analogy: [('grill', 0.38019317388534546), ('bbq', 0.35111916065216064), ('burger', 0.34664222598075867), ('steaks', 0.340492844581604), ('lasagna', 0.3275046646595001), ('reuben', 0.32319068908691406), ('ribs', 0.3157573342323303), ('broiling', 0.3092014491558075), ('fajitas', 0.30499404668807983), ('indoor', 0.30170127749443054)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Doesn't match: {dm}\")\n",
    "print(f\"Common terms: {common_word}\")\n",
    "print(f\"Analogy: {analogy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models vectors to measure a shift in meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tv = pymongo.MongoClient()['tmdb']\n",
    "tvseries = db_tv['tvseries']\n",
    "\n",
    "q = {}\n",
    "tv_corpus = []\n",
    "size = tvseries.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for tvs in tvseries.find(q).limit(limit):\n",
    "    try:\n",
    "        tv_corpus.append(word_tokenize(tvs['overview'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walter', 'white', ',', 'a', 'new', 'mexico']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_corpus[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_model = Word2Vec(sentences=tv_corpus, vector_size=100, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ultimate', 0.9939123392105103),\n",
       " ('phone', 0.9924060702323914),\n",
       " ('scoops', 0.9913731217384338),\n",
       " ('maria', 0.9903990030288696),\n",
       " ('clarity', 0.9899325966835022),\n",
       " ('miami', 0.9899277091026306),\n",
       " ('skywalker', 0.9898713827133179),\n",
       " ('mace', 0.9897506237030029),\n",
       " ('mei', 0.9896987676620483),\n",
       " ('fan', 0.9895042777061462)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_model.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sister', 0.7777917981147766),\n",
       " ('daughter', 0.7555021643638611),\n",
       " ('boyfriend', 0.7444507479667664),\n",
       " ('niece', 0.7425244450569153),\n",
       " ('wife', 0.7346354722976685),\n",
       " ('dd', 0.7327642440795898),\n",
       " ('dad', 0.7315940856933594),\n",
       " ('fiance', 0.728138267993927),\n",
       " ('son', 0.7229806780815125),\n",
       " ('brother-in-law', 0.7174952626228333)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('brother')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring semantic shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian corpus: 4920, Chinese corpus: 4871\n"
     ]
    }
   ],
   "source": [
    "italian_q = {'search_terms': 'italian'}\n",
    "chinese_q = {'search_terms': 'chinese'}\n",
    "limit = 5_000\n",
    "italian_corpus = []\n",
    "chinese_corpus = []\n",
    "\n",
    "for q, c in [(italian_q, italian_corpus), (chinese_q, chinese_corpus)]:\n",
    "    for doc in recipes.find(q).limit(limit):\n",
    "        try:\n",
    "            tokens = word_tokenize(doc['description'].lower())\n",
    "            c.append(tokens)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "print(f\"Italian corpus: {len(italian_corpus)}, Chinese corpus: {len(chinese_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus = italian_corpus + chinese_corpus\n",
    "m0 = Word2Vec(sentences=main_corpus, vector_size=100, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.658794105052948),\n",
       " ('supper', 0.6461277008056641),\n",
       " ('lunch', 0.5710224509239197),\n",
       " ('night', 0.5304909944534302),\n",
       " ('multitasking', 0.5162426829338074),\n",
       " ('snack', 0.5153259038925171),\n",
       " ('brunch', 0.5014334321022034),\n",
       " ('dinners', 0.49514809250831604),\n",
       " ('cocktail', 0.4790607988834381),\n",
       " ('take-alongs', 0.47777262330055237)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m0.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the global model to specific sub-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_it = copy.deepcopy(m0)\n",
    "m_ch = copy.deepcopy(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7022862, 9992950)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.train(italian_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)\n",
    "m_ch.train(chinese_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.5930757522583008),\n",
       " ('supper', 0.546654224395752),\n",
       " ('snack', 0.5166825652122498),\n",
       " ('cocktail', 0.4984075427055359),\n",
       " ('multitasking', 0.4908176362514496),\n",
       " ('hostess', 0.46310943365097046),\n",
       " ('take-alongs', 0.45842233300209045),\n",
       " ('brunch', 0.45402300357818604),\n",
       " ('lunch', 0.45054692029953003),\n",
       " ('picnics', 0.4429490268230438)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.6648055911064148),\n",
       " ('supper', 0.5536285638809204),\n",
       " ('multitasking', 0.49264785647392273),\n",
       " ('night', 0.48381927609443665),\n",
       " ('lunch', 0.48119181394577026),\n",
       " ('snack', 0.46922823786735535),\n",
       " ('crowd', 0.43798351287841797),\n",
       " ('entree', 0.41686946153640747),\n",
       " ('entertaining', 0.4125153720378876),\n",
       " ('unexpected', 0.40737923979759216)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_ch.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'spaghetti'\n",
    "v0, vit, vch = m0.wv.get_vector(word), m_it.wv.get_vector(word), m_ch.wv.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to IT: 0.05137380935261615\n",
      "Moving to CH: 0.05520601573640993\n",
      "Moving from IT to CH: 0.09825096112418708\n"
     ]
    }
   ],
   "source": [
    "print(f\"Moving to IT: {distance.cosine(vit, v0)}\")\n",
    "print(f\"Moving to CH: {distance.cosine(vch, v0)}\")\n",
    "print(f\"Moving from IT to CH: {distance.cosine(vch, vit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
