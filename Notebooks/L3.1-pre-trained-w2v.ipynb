{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Master Degree in Computer Science and Data Science for Economics\n",
    "\n",
    "# Word2Vec resources\n",
    "## Example of using W2V to check for semantic shifts\n",
    "\n",
    "### Alfio Ferrara\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functionalities of `gensim` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()['cousine']\n",
    "recipes = db['foodcom']\n",
    "\n",
    "q = {}\n",
    "recipe_corpus = []\n",
    "size = recipes.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for recipe in recipes.find(q).limit(limit):\n",
    "    try:\n",
    "        recipe_corpus.append(word_tokenize(recipe['description'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'love', 'grits', ',', 'this', 'is', 'another', 'good', 'way', 'to', 'serve', 'them', '.', 'a', 'great', 'alternative', 'to', 'a', 'baked', 'potato', 'when', 'served', 'with', 'grilled', 'steak', 'or', 'chicken', '.', 'i', 'belive', 'this', 'recipe', 'could', 'be', 'made', 'with', 'instant', 'grits.the', '2', '1/2', 'hours', 'for', 'refrigeration', 'is', 'not', 'include', 'in', 'time', '.', 'the', 'recipe', 'comes', 'from', 'tast', 'of', 'home', \"'s\", 'light', 'and', 'tasty', '.']\n"
     ]
    }
   ],
   "source": [
    "print(recipe_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_model = Word2Vec(sentences=recipe_corpus, vector_size=300, window=5, \n",
    "                        min_count=1, workers=8, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supper', 0.6818049550056458),\n",
       " ('meal', 0.5486781001091003),\n",
       " ('brunch', 0.478179007768631),\n",
       " ('entertaining', 0.4659525454044342),\n",
       " ('entree', 0.45991620421409607),\n",
       " ('appetizer', 0.45308664441108704),\n",
       " ('dinners', 0.44084885716438293),\n",
       " ('lunch', 0.41908538341522217),\n",
       " ('picnic', 0.4138370752334595),\n",
       " ('gathering', 0.41037610173225403)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compositionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = recipe_model.wv.doesnt_match(['pasta', 'spaghetti', 'noodles', 'apple'])\n",
    "common = recipe_model.wv.get_mean_vector(['pasta', 'spaghetti', 'noodles', 'risotto'])\n",
    "common_word = recipe_model.wv.similar_by_vector(common)\n",
    "analogy = recipe_model.wv.most_similar(positive=['pizza', 'steak'], negative=['tomato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doesn't match: apple\n",
      "Common terms: [('pasta', 0.7942464351654053), ('noodles', 0.7896058559417725), ('spaghetti', 0.7262192368507385), ('lasagna', 0.6067780256271362), ('risotto', 0.5812796950340271), ('polenta', 0.5663238167762756), ('fettuccine', 0.5209885835647583), ('couscous', 0.5188214778900146), ('penne', 0.5112525820732117), ('steamed', 0.49540385603904724)]\n",
      "Analogy: [('steaks', 0.381004273891449), ('grill', 0.37874242663383484), ('ribs', 0.36033061146736145), ('burger', 0.35143524408340454), ('roast', 0.3302801549434662), ('flank', 0.3176497519016266), ('kransky', 0.3118528425693512), ('bbq', 0.3111320734024048), ('sandwich', 0.31009718775749207), ('rack', 0.30922338366508484)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Doesn't match: {dm}\")\n",
    "print(f\"Common terms: {common_word}\")\n",
    "print(f\"Analogy: {analogy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models vectors to measure a shift in meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tv = pymongo.MongoClient()['tmdb']\n",
    "tvseries = db_tv['tvseries']\n",
    "\n",
    "q = {}\n",
    "tv_corpus = []\n",
    "size = tvseries.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for tvs in tvseries.find(q).limit(limit):\n",
    "    try:\n",
    "        tv_corpus.append(word_tokenize(tvs['overview'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walter', 'white', ',', 'a', 'new', 'mexico']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_corpus[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_model = Word2Vec(sentences=tv_corpus, vector_size=100, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ultimate', 0.9926475286483765),\n",
       " ('watch', 0.9902042150497437),\n",
       " ('sect', 0.9894689917564392),\n",
       " ('robotechnology', 0.987846314907074),\n",
       " ('tenured', 0.9876978397369385),\n",
       " ('emmy', 0.9876712560653687),\n",
       " ('fort', 0.9876428246498108),\n",
       " ('discussion', 0.9875051975250244),\n",
       " ('head', 0.9873998165130615),\n",
       " ('phone', 0.9871098399162292)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_model.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boyfriend', 0.7582463026046753),\n",
       " ('sister', 0.7582341432571411),\n",
       " ('daughter', 0.7382907271385193),\n",
       " ('fiance', 0.7378897070884705),\n",
       " ('dad', 0.7274300456047058),\n",
       " ('niece', 0.7191648483276367),\n",
       " ('wife', 0.7175347208976746),\n",
       " ('dd', 0.7085770964622498),\n",
       " ('father', 0.7035520076751709),\n",
       " ('son', 0.6993942260742188)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('brother')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring semantic shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian corpus: 4920, Chinese corpus: 4871\n"
     ]
    }
   ],
   "source": [
    "italian_q = {'search_terms': 'italian'}\n",
    "chinese_q = {'search_terms': 'chinese'}\n",
    "limit = 5_000\n",
    "italian_corpus = []\n",
    "chinese_corpus = []\n",
    "\n",
    "for q, c in [(italian_q, italian_corpus), (chinese_q, chinese_corpus)]:\n",
    "    for doc in recipes.find(q).limit(limit):\n",
    "        try:\n",
    "            tokens = word_tokenize(doc['description'].lower())\n",
    "            c.append(tokens)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "print(f\"Italian corpus: {len(italian_corpus)}, Chinese corpus: {len(chinese_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus = italian_corpus + chinese_corpus\n",
    "m0 = Word2Vec(sentences=main_corpus, vector_size=100, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.616359293460846),\n",
       " ('supper', 0.6161196231842041),\n",
       " ('lunch', 0.6113405823707581),\n",
       " ('night', 0.5285757780075073),\n",
       " ('snack', 0.504379153251648),\n",
       " ('brunch', 0.4708978533744812),\n",
       " ('starter', 0.4672921299934387),\n",
       " ('dinners', 0.4645257294178009),\n",
       " ('week', 0.46191006898880005),\n",
       " ('multitasking', 0.4587549865245819)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m0.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the global model to specific sub-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_it = copy.deepcopy(m0)\n",
    "m_ch = copy.deepcopy(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7020588, 9992950)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.train(italian_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)\n",
    "m_ch.train(chinese_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.5755337476730347),\n",
       " ('supper', 0.5337268710136414),\n",
       " ('snack', 0.5199846029281616),\n",
       " ('lunch', 0.4908667206764221),\n",
       " ('hostess', 0.47211456298828125),\n",
       " ('multitasking', 0.4704848527908325),\n",
       " ('starter', 0.4685384929180145),\n",
       " ('brunch', 0.44351819157600403),\n",
       " ('take-alongs', 0.4351400136947632),\n",
       " ('cocktail', 0.4314136207103729)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.6126087307929993),\n",
       " ('supper', 0.5882522463798523),\n",
       " ('lunch', 0.5128371119499207),\n",
       " ('night', 0.47236335277557373),\n",
       " ('multitasking', 0.45236438512802124),\n",
       " ('snack', 0.4518190324306488),\n",
       " ('hostess', 0.4326353669166565),\n",
       " ('entree', 0.429100900888443),\n",
       " ('crowd', 0.41184669733047485),\n",
       " ('appetiser', 0.40547966957092285)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_ch.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'spaghetti'\n",
    "v0, vit, vch = m0.wv.get_vector(word), m_it.wv.get_vector(word), m_ch.wv.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to IT: 0.055795016512140916\n",
      "Moving to CH: 0.058662352822629216\n",
      "Moving from IT to CH: 0.12499735031046066\n"
     ]
    }
   ],
   "source": [
    "print(f\"Moving to IT: {distance.cosine(vit, v0)}\")\n",
    "print(f\"Moving to CH: {distance.cosine(vch, v0)}\")\n",
    "print(f\"Moving from IT to CH: {distance.cosine(vch, vit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
