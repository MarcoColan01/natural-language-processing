{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Master Degree in Computer Science and Data Science for Economics\n",
    "\n",
    "# GPT\n",
    "\n",
    "### Elisabetta Rocchetti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "import einops\n",
    "import numpy as np\n",
    "import circuitsvis as cv\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "hooked_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch\n",
    "\n",
    "Today we will see how to code a decoder-only transformer from scratch. This tutorial is adapted from [this](https://arena-ch1-transformers.streamlit.app/%5B1.1%5D_Transformer_from_Scratch) beautiful course, so if you are interested in getting deeper knowledge in this topic just go there and complete the whole tutorial (also, if you missed anythung during this lecture, you can go there and catch up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and Outputs - recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-overview-new.png\" width = \"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers offer multiple functions that apparently do the same thing and I have to read the [documentation](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) each time I have to choose which one to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The raccoon sat on the mat.\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Token (ids): {token_ids}\")\n",
    "print(f\"Tokens (string): {tokenizer.tokenize(text)}\")\n",
    "print(f\"Text string: {tokenizer.decode(token_ids, skip_special_tokens= False)}\")\n",
    "print(f\"Stuff to input a model: {tokenizer(text, return_tensors='pt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises: \n",
    "\n",
    "- try to tokenize texts beginning with a capital letter or a space: what happens?\n",
    "- try to tokenize long sequences of numbers or arithmetic operations: what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs\n",
    "\n",
    "Let's generate text with out `gpt-2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Once upon a\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    output_logits = gpt2(**input_ids)[\"logits\"]\n",
    "print(f\"Logits: {output_logits}\")\n",
    "print(f\"Logits shape: {output_logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have:\n",
    "\n",
    "- the batch dimension, which has 1 element, given that we have one sentence\n",
    "- the sequence length dimension, which contains 3 tokens\n",
    "- the vocabulary length dimension, which contains 50257 logits, one for each vocab\n",
    "\n",
    "The model has predicted a logit vector for each token in our sentence. We can convert them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_probas = output_logits.softmax(dim=-1)\n",
    "print(f\"Probabilites over vocabulary: {output_probas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select which is the most probable **next** token at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_next_tokens = tokenizer.batch_decode(output_logits.argmax(dim=-1)[0])\n",
    "print(list(zip(tokenizer.tokenize(text), most_likely_next_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this output, the next token will be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = output_logits[0, -1].argmax(dim=-1)\n",
    "next_char = tokenizer.decode(next_token)\n",
    "print(\n",
    "    \"The next token is:\", repr(next_char)\n",
    ")  # repr is to show special tokens and spaces\n",
    "print(\"How the sentence becomes: \", text + next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is repeated iteratively, appending the next token predicion at the end of the original sentence, and giving the updated sentence to `gpt-2`again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text\n",
    "text = \"Once upon a\"\n",
    "# Convert text to tensor format\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "print(\"Generating text...\\n\")\n",
    "# Generate 10 characters iteratively\n",
    "for i in range(10):\n",
    "    with torch.inference_mode():\n",
    "        # Get model predictions\n",
    "        output_logits = gpt2(**tokens).logits\n",
    "        # Select the most likely next token\n",
    "        next_token = output_logits[0, -1].argmax(dim=-1)\n",
    "        # Decode the token to a character\n",
    "        next_char = tokenizer.decode(next_token)\n",
    "    # Display the sequence so far\n",
    "    current_text = tokenizer.decode(tokens[\"input_ids\"][0])  # Reconstruct the string\n",
    "    print(f\"Generation step {i+1}:\")\n",
    "    print(f\"Sequence so far: {current_text!r}\")\n",
    "    print(f\"{tokens['input_ids'].shape[-1]+1}th char = {next_char!r}\\n\")\n",
    "    # Append the new character and re-tokenize\n",
    "    text += next_char\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "print(\"Final text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: iteratively generate a sentence, and stop when the model predict the end of sequence token as next token, considering that every special token for `gpt-2`is mapped to `<|endoftext|>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand things by coding them is pretty convenient. Here you will see reported pieces of code inspired or copied from the [same tutorial](https://arena-ch1-transformers.streamlit.app/%5B1.1%5D_Transformer_from_Scratch) I have linked above. The following code shows how to implement:\n",
    "\n",
    "- LayerNorm (transforming the input to have zero mean and unit variance)\n",
    "- Positional embedding (a lookup table from position indices to residual stream vectors)\n",
    "- Attention (the method of computing attention patterns for residual stream vectors)\n",
    "- MLP (the collection of linear and nonlinear transformations which operate on each residual stream vector in the same way)\n",
    "- Embedding (a lookup table from tokens to residual stream vectors)\n",
    "- Unembedding (a matrix for converting residual stream vectors into a distribution over tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a decoder-only architecture like the one in GPT as being constituted by 3 main modules:\n",
    "\n",
    "1) Embedding module\n",
    "2) Transformer block, with attention and multi layered perceptrons (MLP)\n",
    "3) Unembedding module\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-new.png\" width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Embedding module\n",
    "\n",
    "Think about these two modules as a 2-step lookup table:\n",
    "\n",
    "1) mapping tokens to integers (as we saw previously by using the tokenizer)\n",
    "2) mapping integer to vectors (learnt during the training phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer blocks\n",
    "\n",
    "After having your tokens as vectors, you would expect to work with a tensor $x_0$ of shape `[batch, seq_len, d_model]` where :\n",
    "\n",
    "- `batch` is the dimension referring to the number of sequences that are being processed at the same time\n",
    "- `seq_len` is the length of each sequence in the batch, thus how many tokens it contains. Usually, you will have to make every sequence of the same length to allow for this batching to work.\n",
    "- `d_model` is the length of embedding vectors as processed by the model, you can refer to this as `hidden_size` too\n",
    "\n",
    "This tensor will enter a series of Transformer blocks containing attention heads followed by MLP.\n",
    "\n",
    "**Attention**. These modules have the power of moving information from *prior* positions to the current token. Note that you are dealing with causal attention, thus a token at position $i$ *cannot* give attention to tokens at positions greater than $i$ (although it can *receive* attention from them). Each attention layer has `n_heads` attention heads with distinct attention patters, indicating how much attention a token $i$ (*destination* token) give to previous tokens $j$ with $j<i$ (*source* tokens). Note: if this terminology is strange, ask the lecturer to explain it again!\n",
    "\n",
    "It can be useful to know that we expect attention patters to be tensors of shape `[batch, n_head, seq_len, seq_len]`, with each patter putting into relation each token in the sequence with every other previous token.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-new.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = \"Once upon a time, there was a fox who lived in a forest.\"\n",
    "tokens = hooked_gpt2.to_tokens(reference_text).to(device)\n",
    "logits, cache = hooked_gpt2.run_with_cache(tokens)\n",
    "html = cv.attention.attention_pattern(\n",
    "    tokens=hooked_gpt2.to_str_tokens(reference_text),\n",
    "    attention=cache[\"pattern\", 3][0][7],\n",
    ")\n",
    "styled_html = f\"\"\"\n",
    "<div style=\"width:800px; font-size:16px;\">\n",
    "    {html}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(styled_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP**. These are standard neural networks with one hidden layer and nonlinear activation functions (e.g. GELU). If attention has moved information among tokens, MLPs process the moved information.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-mlp-new-2.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unembedding\n",
    "This module is just mapping `[batch, seq_len, d_model]` tensors to `[batch, seq_len, d_vocab]`, which is the dimensionality for out outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technicalities\n",
    "\n",
    "- at the beginnig of each layer, there is a normalization step (each input vector will have mean 0 and standard deviation 1)\n",
    "- we use positional embeddings to inform the network about the absolute positions of tokens (imagine an attention patter which does not account for the fact that nearby tokens are more relevant!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fix the dimensionality of the tensors we are going to work with. Since we will take learnt parameters and activations from the pretrained model, we need to make sure that we have the same dimensions as `gpt-2` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt2)\n",
    "print(gpt2.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Once upon a time, \"\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "tokens = tokenizer(sequence, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "print(\"Tokenized sequence:\", tokenized_sequence)\n",
    "print(\"Token IDs:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1  # starting with only one batch (thus 1 sentence)\n",
    "seq_len = len(tokenized_sequence)  # 6\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    n_ctx: int = gpt2.config.n_ctx  # 1024\n",
    "    d_model: int = gpt2.config.n_embd  # hidden size, or embedding dimension\n",
    "    n_heads: int = gpt2.config.n_head  # number of attention heads\n",
    "    n_layers: int = gpt2.config.n_layer  # number of transformer blocks\n",
    "    d_mlp: int = 4 * d_model  # MLP hidden size, 3072\n",
    "    d_head: int = d_model // n_heads  # dimension of each attention head, 64\n",
    "    layer_norm_eps: float = gpt2.config.layer_norm_epsilon  # layer norm epsilon\n",
    "    d_vocab: int = gpt2.config.vocab_size  # number of tokens in the vocabulary\n",
    "    init_range: float = (\n",
    "        gpt2.config.initializer_range\n",
    "    )  # initialization range for weights\n",
    "    debug: bool = True\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output:\", output)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output:\", output)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer (5-10 mins)\n",
    "\n",
    "This layer takes as input a sequence of integers (output by the tokenizer) and has as output a tensor of shape `[batch, seq_len, d_model]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, int_tokens: Int[Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # just a mapping from int tokens to float vectors\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int_test(Embed, [batch, seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional embeddings layer (10-15 mins)\n",
    "\n",
    "This layer is just the same as the previous ones, but the input is not a sequence of token ids but a sequence of integers representing the position of tokens in the sentence. GPT uses learnt positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # complete here\n",
    "\n",
    "    def forward(\n",
    "        self, int_tokens: Int[Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # take first seq_len learnt positional embeddings\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int_test(PosEmbed, [batch, seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm (10-15 mins)\n",
    "\n",
    "The next module is the layer normalization. This module:\n",
    "\n",
    "* Makes mean 0\n",
    "* Normalizes to have variance 1\n",
    "* Scales with learned weights\n",
    "* Translates with learned bias\n",
    "\n",
    "Use the PyTorch [LayerNorm documentation](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) as a reference. A few more notes:\n",
    "\n",
    "* The layernorm implementation always has `affine=True`, i.e. you do learn parameters $\\gamma$ and $\\beta$.\n",
    "* Remember that, after the centering and normalization, each vector of length `d_model` in your input should have mean 0 and variance 1.\n",
    "* As the PyTorch documentation page says, your variance should be computed using `unbiased=False`.\n",
    "* The `layer_norm_eps` argument in your config object corresponds to the $\\epsilon$ term in the PyTorch documentation (it is included to avoid division-by-zero errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # complete here\n",
    "\n",
    "    def forward(\n",
    "        self, embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # compute mean\n",
    "        # compute standard deviation + eps\n",
    "        # compute normalized embedding\n",
    "        pass\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [batch, seq_len, cfg.d_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention (30-45 mins)\n",
    "\n",
    "* **Step 1:** Produce an attention pattern - for each destination token, probability distribution over previous tokens (including current token)\n",
    "    * Linear map from input -> query, key shape `[batch, seq_len, head_index, d_head]`\n",
    "    * Dot product every *pair* of queries and keys to get attn_scores `[batch, head_index, query_pos, key_pos]` (query = dest, key = source)\n",
    "    * **Scale** and mask `attn_scores` to make it lower triangular, i.e. causal\n",
    "    * Softmax along the `key_pos` dimension, to get a probability distribution for each query (destination) token - this is our attention pattern!\n",
    "* **Step 2:** Move information from source tokens to destination token using attention pattern (move = apply linear map)\n",
    "    * Linear map from input -> value `[batch, key_pos, head_index, d_head]`\n",
    "    * Mix along the `key_pos` with attn pattern to get `z`, which is a weighted average of the value vectors `[batch, query_pos, head_index, d_head]`\n",
    "    * Map to output, `[batch, position, d_model]` (position = query_pos, we've summed over all heads)\n",
    "\n",
    "Note - when we say **scale**, we mean dividing by `sqrt(d_head)`. The purpose of this is to avoid vanishing gradients (which is a big problem when we're dealing with a function like softmax - if one of the values is much larger than all the others, the probabilities will be close to 0 or 1, and the gradients will be close to 0).\n",
    "\n",
    "Below is a much larger, more detailed version of the attention head diagram from earlier. This should give you an idea of the actual tensor operations involved. A few clarifications on this diagram:\n",
    "\n",
    "* Whenever there is a third dimension shown in the pictures, this refers to the `head_index` dimension. We can see that all operations within the attention layer are done independently for each head.\n",
    "* The objects in the box are activations; they have a batch dimension (for simplicity, we assume the batch dimension is 1 in the diagram). The objects to the right of the box are our parameters (weights and biases); they have no batch dimension.\n",
    "* We arrange the keys, queries and values as `(batch, seq_pos, head_idx, d_head)`, because the biases have shape `(head_idx, d_head)`, so this makes it convenient to add the biases (recall the rules of array broadcasting!).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-21.png\" width=\"1400\">\n",
    "\n",
    "A couple of notes / hints:\n",
    "\n",
    "* Don't forget the attention score scaling (this should come before the masking).\n",
    "* You can use `torch.where`, or the `torch.masked_fill` function when masking the attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # complete here\n",
    "\n",
    "    def forward(\n",
    "        self, embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # compute K, Q, V projections\n",
    "        # compute attention scores\n",
    "        # scale and mask attention scores (causal attention)\n",
    "        # softmax attention scores\n",
    "        # compute weighted sum of values\n",
    "        # compute output projection\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_float_test(Attention, [batch, seq_len, cfg.d_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (10-15 mins)\n",
    "\n",
    "Next, you should implement the MLP layer, which consists of:\n",
    "\n",
    "* A linear layer, with weight `W_in`, bias `b_in`\n",
    "* A nonlinear function (we usually use GELU; the function `gelu_new` has been imported for this purpose)\n",
    "* A linear layer, with weight `W_out`, bias `b_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_new(\n",
    "    input: Float[torch.Tensor, \"batch pos d_mlp\"],\n",
    ") -> Float[torch.Tensor, \"batch pos d_mlp\"]:\n",
    "    # Implementation of GeLU used by GPT2 - subtly different from PyTorch's\n",
    "    return (\n",
    "        0.5\n",
    "        * input\n",
    "        * (\n",
    "            1.0\n",
    "            + torch.tanh(\n",
    "                np.sqrt(2.0 / np.pi) * (input + 0.044715 * torch.pow(input, 3.0))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, cgf: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # complete here\n",
    "\n",
    "    def forward(\n",
    "        self, embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # compute in projection\n",
    "        # apply activation\n",
    "        # compute out projection\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_float_test(MLP, [batch, seq_len, cfg.d_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer block: asssembling everything together! (10 mins)\n",
    "\n",
    "Now, we can put together the attention, MLP and layernorms into a single transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # complete here\n",
    "\n",
    "    def forward(\n",
    "        self, input_embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # normalize input\n",
    "        # compute attention and add skip connection\n",
    "        # normalize embedding\n",
    "        # compute MLP and add skip connection\n",
    "        # return output\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_float_test(TransformerBlock, [batch, seq_len, cfg.d_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unembedding (10 mins)\n",
    "\n",
    "The unembedding is jus a linear layer (with weight `W_U` and bias `b_U`). This is basically a map from embeddings to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # complete here\n",
    "\n",
    "    def forward(\n",
    "        self, embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        # compute logits\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_float_test(Unembed, [batch, seq_len, cfg.d_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer (10 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # complete here\n",
    "\n",
    "    def forward(\n",
    "        self, input_tokens: Int[Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        # compute embeddings + positional embeddings\n",
    "        # compute transformer blocks outputs\n",
    "        # normalize output\n",
    "        # compute logits\n",
    "        pass\n",
    "\n",
    "    def load_gpt2_weights(self, gpt2: GPT2LMHeadModel) -> None:\n",
    "        state_dict = {}\n",
    "\n",
    "        state_dict[\"embed.W_E\"] = gpt2.transformer.wte.weight\n",
    "        state_dict[\"pos_embed.W_pos\"] = gpt2.transformer.wpe.weight\n",
    "\n",
    "        for l in range(cfg.n_layers):\n",
    "            state_dict[f\"blocks.{l}.ln1.w\"] = gpt2.transformer.h[l].ln_1.weight\n",
    "            state_dict[f\"blocks.{l}.ln1.b\"] = gpt2.transformer.h[l].ln_1.bias\n",
    "\n",
    "            # In GPT-2, q,k,v are produced by one big linear map, whose output is\n",
    "            # concat([q, k, v])\n",
    "            W = gpt2.transformer.h[l].attn.c_attn.weight\n",
    "            W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=1)\n",
    "            W_Q = einops.rearrange(W_Q, \"m (i h)->i m h\", i=cfg.n_heads)\n",
    "            W_K = einops.rearrange(W_K, \"m (i h)->i m h\", i=cfg.n_heads)\n",
    "            W_V = einops.rearrange(W_V, \"m (i h)->i m h\", i=cfg.n_heads)\n",
    "\n",
    "            state_dict[f\"blocks.{l}.attn.W_Q\"] = W_Q\n",
    "            state_dict[f\"blocks.{l}.attn.W_K\"] = W_K\n",
    "            state_dict[f\"blocks.{l}.attn.W_V\"] = W_V\n",
    "\n",
    "            qkv_bias = gpt2.transformer.h[l].attn.c_attn.bias\n",
    "            qkv_bias = einops.rearrange(\n",
    "                qkv_bias,\n",
    "                \"(qkv index head)->qkv index head\",\n",
    "                qkv=3,\n",
    "                index=cfg.n_heads,\n",
    "                head=cfg.d_head,\n",
    "            )\n",
    "            state_dict[f\"blocks.{l}.attn.b_Q\"] = qkv_bias[0]\n",
    "            state_dict[f\"blocks.{l}.attn.b_K\"] = qkv_bias[1]\n",
    "            state_dict[f\"blocks.{l}.attn.b_V\"] = qkv_bias[2]\n",
    "\n",
    "            W_O = gpt2.transformer.h[l].attn.c_proj.weight\n",
    "            W_O = einops.rearrange(W_O, \"(i h) m->i h m\", i=cfg.n_heads)\n",
    "            state_dict[f\"blocks.{l}.attn.W_O\"] = W_O\n",
    "            state_dict[f\"blocks.{l}.attn.b_O\"] = gpt2.transformer.h[l].attn.c_proj.bias\n",
    "\n",
    "            state_dict[f\"blocks.{l}.ln2.w\"] = gpt2.transformer.h[l].ln_2.weight\n",
    "            state_dict[f\"blocks.{l}.ln2.b\"] = gpt2.transformer.h[l].ln_2.bias\n",
    "\n",
    "            W_in = gpt2.transformer.h[l].mlp.c_fc.weight\n",
    "            state_dict[f\"blocks.{l}.mlp.W_in\"] = W_in\n",
    "            state_dict[f\"blocks.{l}.mlp.b_in\"] = gpt2.transformer.h[l].mlp.c_fc.bias\n",
    "\n",
    "            W_out = gpt2.transformer.h[l].mlp.c_proj.weight\n",
    "            state_dict[f\"blocks.{l}.mlp.W_out\"] = W_out\n",
    "            state_dict[f\"blocks.{l}.mlp.b_out\"] = gpt2.transformer.h[l].mlp.c_proj.bias\n",
    "        state_dict[\"unembed.W_U\"] = gpt2.lm_head.weight.T\n",
    "\n",
    "        state_dict[\"ln_final.w\"] = gpt2.transformer.ln_f.weight\n",
    "        state_dict[\"ln_final.b\"] = gpt2.transformer.ln_f.bias\n",
    "        self.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int_test(GPT, [batch, seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try GPT out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_gpt2 = GPT(Config(debug=False)).to(device)\n",
    "# demo_gpt2.load_gpt2_weights(gpt2)\n",
    "demo_gpt2.load_state_dict(hooked_gpt2.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text\n",
    "text = \"Once upon a\"\n",
    "# Convert text to tensor format\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "print(\"Generating text...\\n\")\n",
    "# Generate 10 characters iteratively\n",
    "for i in range(20):\n",
    "    with torch.inference_mode():\n",
    "        # Get model predictions\n",
    "        output_logits = demo_gpt2(tokens[\"input_ids\"])\n",
    "        # Select the most likely next token\n",
    "        next_token = output_logits[0, -1].argmax(dim=-1)\n",
    "        # Decode the token to a character\n",
    "        next_char = tokenizer.decode(next_token)\n",
    "    # Display the sequence so far\n",
    "    current_text = tokenizer.decode(tokens[\"input_ids\"][0])  # Reconstruct the string\n",
    "    print(f\"Generation step {i+1}:\")\n",
    "    print(f\"Sequence so far: {current_text!r}\")\n",
    "    print(f\"{tokens['input_ids'].shape[-1]+1}th char = {next_char!r}\\n\")\n",
    "    # Append the new character and re-tokenize\n",
    "    text += next_char\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "print(\"Final text:\", text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching-gpt-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
